{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98527646-06a3-4bc0-94f5-bf41a407025a",
   "metadata": {},
   "source": [
    "# Text Summarization: 03b. modeling with instruction fine tuning and LoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9655683-3530-4263-9392-7942c1d2e666",
   "metadata": {},
   "source": [
    "* QLORA paper: https://arxiv.org/abs/2305.14314\n",
    "* example notebook: https://blog.ovhcloud.com/fine-tuning-llama-2-models-using-a-single-gpu-qlora-and-ai-notebooks/\n",
    "* from bits and bytes https://huggingface.co/blog/4bit-transformers-bitsandbytes\n",
    "    * colab notebook: https://colab.research.google.com/drive/1VoYNfYDKcKRQRor98Zbf2-9VQTtGJ24k?usp=sharing#scrollTo=gkIcwsSU01EB \n",
    "    * Old blog plost: https://huggingface.co/blog/hf-bitsandbytes-integration\n",
    "    * Old LLM.int8(): https://arxiv.org/abs/2208.07339\n",
    " \n",
    "* When cuda error after suspended: https://discuss.pytorch.org/t/userwarning-cuda-initialization-cuda-unknown-error-this-may-be-due-to-an-incorrectly-set-up-environment-e-g-changing-env-variable-cuda-visible-devices-after-program-start-setting-the-available-devices-to-be-zero/129335/4\n",
    "\n",
    "```sudo rmmod nvidia_uvm```\n",
    "\n",
    "```sudo modprobe nvidia_uvm```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d0fea7-8ff7-4931-800b-df25dabc185b",
   "metadata": {},
   "source": [
    "* **steps**\n",
    "1. Load dataset\n",
    "2. create bnb config\n",
    "3. Load model with bnb config on quantization info\n",
    "4. find modules for LORA\n",
    "5. Create LORA config and wrap the model\n",
    "6. Config train\n",
    "7. Train model\n",
    "8. Save model\n",
    "9. clear memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029866f2-c4b1-4281-8d4f-ef04c620b2d1",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24955666-c2e7-468c-b958-0d252710a771",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/stephen/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from datasets import DatasetDict, load_from_disk\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, GenerationConfig, PreTrainedModel\n",
    "import torch\n",
    "import evaluate\n",
    "import re\n",
    "import numpy as np\n",
    "import time\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk \n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ef2f6dd-95c7-4da2-a1f2-2a8f5309d4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to install: bitsandbytes, peft, scipy (bitsandbytes needs it)\n",
    "\n",
    "import bitsandbytes as bnb\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, AutoPeftModelForSeq2SeqLM, TaskType\n",
    "from transformers import ( \n",
    "    set_seed, Seq2SeqTrainingArguments, BitsAndBytesConfig, \n",
    "    DataCollatorForLanguageModeling, Seq2SeqTrainer, DataCollatorForSeq2Seq\n",
    ")\n",
    "import accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fef7a0ab-42fc-40a8-a7ee-3cc1a974155e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set variables\n",
    "SEED = 1234\n",
    "curr_path = os.getcwd()\n",
    "path_raw = os.path.join(curr_path, '../data/raw', 'bill_summary_us')\n",
    "# dataset_name = 'vgoldberg/longform_article_summarization'\n",
    "dataset_name = 'dreamproit/bill_summary_us'\n",
    "path_preprocessed = os.path.join(curr_path, '../data/preprocessed', 'bill_summary_us_single_section')\n",
    "\n",
    "path_predictions = os.path.join(\n",
    "    curr_path, '../models/', \n",
    "    'flan_t5_base_pretrained', \n",
    "    'predictions', 'bill_summary_us_single_section'\n",
    ")\n",
    "\n",
    "path_predictions_lora = os.path.join(\n",
    "    curr_path, '../models/', \n",
    "    'flan_t5_finetuned_lora', \n",
    "    'predictions', 'bill_summary_us_single_section'\n",
    ")\n",
    "\n",
    "max_input_length = 1024\n",
    "max_target_length = 512\n",
    "\n",
    "num_beams = 5\n",
    "max_summary_ratio = 1.0\n",
    "\n",
    "model_checkpoint = 'google/flan-t5-base'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba19bec-64c2-49ad-9426-69216319983e",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63f4b2b4-b043-4bb4-8f50-e9a8a60a1c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_example(example: dict) -> None:\n",
    "    \"\"\"Print example.\n",
    "    :param example: dict, example.\n",
    "    \"\"\"\n",
    "    print(' '.join(['=' * 10, 'text', '=' * 10]))\n",
    "    print(example['text'])\n",
    "    print(' '.join(['=' * 10, 'summary', '=' * 10]))\n",
    "    print(example['summary'])    \n",
    "\n",
    "\n",
    "def remove_missing_data(example: dict, col_list: list[str]) -> bool:\n",
    "    \"\"\"Remove missing data.\n",
    "    :param example: dict, example.\n",
    "    :param col_list, list[str], list of columns.\n",
    "    :return: bool, whether valid (True) or missing data (False). \n",
    "    \"\"\"\n",
    "    flag_value = True\n",
    "    for col in col_list:\n",
    "        flag_value = flag_value and example[col] is not None\n",
    "    return flag_value\n",
    "\n",
    "\n",
    "def count_words(example: dict, col_list: list[str]) -> dict:    \n",
    "    \"\"\"Count words.\n",
    "    :param example: dict, example.\n",
    "    :param col_list, list[str], list of columns.\n",
    "    :return: dict, count dictionary with prefix `count_`.\n",
    "    \"\"\"\n",
    "    count_dict = {}\n",
    "    for col_name in col_list:\n",
    "        col_count = '_'.join(['count', col_name])\n",
    "        word_count = len(example[col_name].split())\n",
    "        count_dict[col_count] = word_count\n",
    "    return count_dict\n",
    "\n",
    "\n",
    "def process_tab(example: dict, col: str) -> dict:    \n",
    "    \"\"\"Process tab. Remove triple tabs then replace a tab with space.\n",
    "    :param example: dict, example.\n",
    "    :param col: str, column name to process.\n",
    "    :return, dict, processed example.\n",
    "    \"\"\"\n",
    "    example[col] = example[col].replace('\\t\\t\\t', '').replace('\\t', ' ')\n",
    "    return example\n",
    "\n",
    "\n",
    "def plot_text_length(df: pd.DataFrame, col_list: list[str]) -> tuple:\n",
    "    \"\"\"Plot historgram of text length.\n",
    "    :param df, pd.DataFrame, input dataframe.\n",
    "    :param col_list: list[str], a list of column names.\n",
    "    :return: tuple, (fig, axis_tuple).\n",
    "        - fig: matplotlib.figure, figure handle.\n",
    "        - axis_tuple: tuple[matplotlib.axes.Axes], a tuple of axes.\n",
    "    \"\"\"\n",
    "    num_col = len(col_list)\n",
    "    fig, axis_tuple = plt.subplots(nrows=1, ncols=num_col, figsize=[6.4 * num_col, 4.8])\n",
    "    for idx, col in enumerate(col_list):\n",
    "        ax = axis_tuple[idx]\n",
    "        _ = df.plot(y=col, kind='hist', bins=100, ax=ax)  \n",
    "\n",
    "    return fig, axis_tuple\n",
    "\n",
    "\n",
    "def find_outliers_cutoff(df: pd.DataFrame, scale: float = 1.5) -> dict[tuple]:\n",
    "    \"\"\"Find outliers cutoff.\n",
    "    :param df, pd.DataFrame, input dataframe.\n",
    "    :param scale: float, scale factor for IQR to define whiskter. Default: 1.5.\n",
    "    :return: dict[tuple], a dictionary of lower and upper cutoff for each column.\n",
    "    \"\"\"\n",
    "    # upper, lower whisker: pd.Series\n",
    "    q3 = df.quantile(q=0.75)\n",
    "    q1 = df.quantile(q=0.25)\n",
    "    iqr = q3 - q1\n",
    "    upper = q3 + scale * iqr\n",
    "    lower = q1 - scale * iqr\n",
    "    \n",
    "    upper_dict = upper.to_dict()\n",
    "    lower_dict = lower.to_dict()\n",
    "    cutoff_dict = {}\n",
    "    for key in upper_dict:\n",
    "        cutoff_dict[key] = (lower_dict[key], upper_dict[key])\n",
    "    return cutoff_dict    \n",
    "\n",
    "\n",
    "def remove_outliers(example: dict, col_list: list, cutoff_dict: dict[tuple]) -> bool:\n",
    "    \"\"\"Remove outliers.\n",
    "    :param example: dict, example.\n",
    "    :param col_list, list[str], list of columns.\n",
    "    :param cutoff_dict: dict[tuple], a dictionary of lower and upper cutoff for each column.\n",
    "    :return: bool, whether valid (True) or outliers (False).\n",
    "    \"\"\"\n",
    "    flag_val = True\n",
    "    for col in col_list:\n",
    "        cutoff_low, cutoff_high = cutoff_dict[col]\n",
    "        flag_val = flag_val and cutoff_low <= example[col] <= cutoff_high\n",
    "    return flag_val\n",
    "\n",
    "\n",
    "def create_prompt(example: dict, col_text: str, prompt_prefix: str) -> dict:\n",
    "    \"\"\"Creat prompt with prompt prefix.\n",
    "    :param example: dict, example.\n",
    "    :param col_text: str, column for text.\n",
    "    :param prompt_prefix: str, prompt prefix for instruction.\n",
    "    :return: dict, example with prompt column.\n",
    "    \"\"\"\n",
    "    example['prompt'] = f'{prompt_prefix}: {example[col_text]}'\n",
    "    return example\n",
    "\n",
    "\n",
    "#  build system to tokenize\n",
    "def tokenize_example(\n",
    "    example: dict, tokenizer: AutoTokenizer, col_prompt: str, col_summary: str, \n",
    "    max_input_length: int, max_target_length: int\n",
    ") -> dict: \n",
    "    \"\"\"Tokenize example.\n",
    "    :param example: dict, example.\n",
    "    :param tokenizer: transformers.AutoTokenizer, tokenizer.\n",
    "    :param col_prompt: str, column name for prompt.\n",
    "    :param col_summary: str, column name for summary (target)\n",
    "    :param max_input_length: int, maximum input token length. \n",
    "    :param max_target_length: int, maximum target token length. \n",
    "    :return: dict, tokens for input ('input_ids') and target ('label').\n",
    "    \"\"\"\n",
    "    model_input = tokenizer(\n",
    "        example[col_prompt], \n",
    "        max_length=max_input_length, \n",
    "        truncation=True,\n",
    "        # return_tensors='pt',\n",
    "    )\n",
    "    label = tokenizer(\n",
    "        # example[col_summary], \n",
    "        text_target=example[col_summary],  \n",
    "        max_length=max_target_length, \n",
    "        truncation=True,\n",
    "        # return_tensors='pt',\n",
    "    )\n",
    "    model_input['label'] = label['input_ids']\n",
    "    return model_input\n",
    "\n",
    "\n",
    "def count_tokens(example: dict, col_list: list[str]) -> dict:    \n",
    "    \"\"\"Count tokens.\n",
    "    :param example: dict, example.\n",
    "    :param col_list: list[str], a list of column names.\n",
    "    :return: dict, token counts with prefix 'count_tokens_'\n",
    "    \"\"\"\n",
    "    count_dict = {}\n",
    "    for col_name in col_list:\n",
    "        col_count = '_'.join(['count_tokens', col_name])\n",
    "        word_count = len(example[col_name])\n",
    "        # print(type(example['text']))  # str\n",
    "        # example[col_count] = len(example['text'].split())\n",
    "        count_dict[col_count] = word_count\n",
    "    return count_dict\n",
    "\n",
    "\n",
    "def preprocessing(ds, col_names, cutoff_dict, col_for_count, tokenizer, max_input_length, max_target_length):   \n",
    "    ds_preprocessed = ( \n",
    "        # 1. remove missing\n",
    "        ds.filter(remove_missing_data, fn_kwargs={'col_list': col_names})\n",
    "        \n",
    "        # 2. filtering\n",
    "        # select only 1 section\n",
    "        .filter(lambda example: example['sections_length'] == 1, num_proc=12)\n",
    "        # remove any zero text length, summary_length\n",
    "        .filter(lambda example: example['text_length'] > 0, num_proc=12)\n",
    "        .filter(lambda example: example['summary_length'] > 0, num_proc=12)\n",
    "        # text_length_outliers, summary_length_outliers\n",
    "        .filter(remove_outliers, fn_kwargs={'col_list': list(cutoff_dict.keys()), 'cutoff_dict': cutoff_dict}, num_proc=12)\n",
    "\n",
    "        # count words\n",
    "        .map(count_words, fn_kwargs={'col_list': col_for_count}, num_proc=12)\n",
    "        \n",
    "        # remove \\t\\t\\t, and replace \\t with ' '\n",
    "        .map(process_tab, fn_kwargs={'col': 'text'}, num_proc=12)\n",
    "        \n",
    "        # create prompt\n",
    "        .map(create_prompt, fn_kwargs={'col_text': 'text', 'prompt_prefix': 'summarize'}, num_proc=12)\n",
    "        \n",
    "        # tokenize: need to specify max_input_length, when number of tokens is greater than default limit (512)\n",
    "        .map(\n",
    "            tokenize_example, \n",
    "            fn_kwargs={\n",
    "                'tokenizer': tokenizer, 'col_prompt': 'prompt', 'col_summary': 'summary', \n",
    "                'max_input_length': max_input_length, 'max_target_length': max_target_length, \n",
    "            },\n",
    "            batched=True, \n",
    "            batch_size=500,\n",
    "            num_proc=12,\n",
    "        )\n",
    "\n",
    "        # count tokens\n",
    "        .map(count_tokens, fn_kwargs={'col_list': ['input_ids', 'label']}, num_proc=12)\n",
    "    )\n",
    "    return ds_preprocessed\n",
    "\n",
    "\n",
    "def inference(\n",
    "    example: dict, tokenizer: AutoTokenizer, model: AutoModelForSeq2SeqLM, \n",
    "    num_beams: int = 5, max_summary_ratio: float = 1.0, device: str = 'cpu'\n",
    ") -> dict:\n",
    "    \"\"\"Make inference.\n",
    "    :param example: dict, example.\n",
    "    :param tokenizer: transformers.AutoTokenizer, tokenizer.\n",
    "    :param model: transformers.AutoModelForSeq2SeqLM, model.\n",
    "    :param num_beams: int, the number of beams for beam search. Default: 5.\n",
    "    :param max_summary_ratio: float, the maximum summary to text ratio. Default: 1.\n",
    "    :param device: str, device to make an inference.\n",
    "    :return: dict, prediction with 'predicted_tokens' and 'prediction' columns.\n",
    "    \"\"\"\n",
    "    # convert dataset into tensor, need to put another dimension\n",
    "    # type torch.LongTensor or torch.int64\n",
    "    # torch batch does not work because length of the sequence varies with examples\n",
    "    input_ids = ( \n",
    "        torch.tensor(example['input_ids'])\n",
    "        .reshape((1, -1))\n",
    "        .to(device)\n",
    "    )\n",
    "\n",
    "    # generate config\n",
    "    generation_config = GenerationConfig(\n",
    "        max_new_tokens=min(\n",
    "            int(max_summary_ratio * example['count_tokens_input_ids']),\n",
    "            max_target_length\n",
    "        ),\n",
    "        num_beams=num_beams, \n",
    "        skip_special_tokens=True, early_stopping=False\n",
    "    )\n",
    "    \n",
    "    # output type: torch.Tensor\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(input_ids, generation_config=generation_config)\n",
    "    # bacl to cpu\n",
    "    if device != 'cpu':\n",
    "        outputs = outputs.to(device)\n",
    "    # decode and remove the initial <pad> and the ending </s>\n",
    "    decoded_outputs = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # assign\n",
    "    example['predicted_tokens'] = outputs[0].tolist()\n",
    "    example['prediction'] = decoded_outputs\n",
    "    # clear cache\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return example\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    # text\n",
    "    predictions, labels = eval_pred\n",
    "    if isinstance(predictions, str):\n",
    "        result = rouge.compute(predictions=[predictions], references=[labels], use_stemmer=True)\n",
    "    else:\n",
    "        result = rouge.compute(predictions=predictions, references=labels, use_stemmer=True)\n",
    "    # round to the 4th decimal\n",
    "    result = {key: round(val, 4) for key, val in result.items()}\n",
    "    return result\n",
    "\n",
    "\n",
    "def compute_metrics_for_training(eval_pred):\n",
    "    # tokens_id\n",
    "    predictions, labels = eval_pred\n",
    "    # Decode generated summaries into text\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    # Replace -100 in the labels as we can't decode them\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    # Decode reference summaries into text\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    # ROUGE expects a newline after each sentence. So, add \\n for new sentence \n",
    "    decoded_preds = [\"\\n\".join(sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
    "    decoded_labels = [\"\\n\".join(sent_tokenize(label.strip())) for label in decoded_labels]\n",
    "    # Compute ROUGE scores\n",
    "    rouge_score = evaluate.load('rouge')\n",
    "    result = rouge_score.compute(\n",
    "        predictions=decoded_preds, references=decoded_labels, use_stemmer=True\n",
    "    )\n",
    "    # round to the 4th decimal\n",
    "    result = {key: round(val, 4) for key, val in result.items()}\n",
    "    return result\n",
    "    \n",
    "    # # Extract the median scores\n",
    "    # result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
    "    # return {k: round(v, 4) for k, v in result.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8fea85-b566-40fc-81f8-a92c0b758eec",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f8c21da-e794-4a47-ab78-29ddd51e1d9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'congress', 'bill_type', 'bill_number', 'bill_version', 'sections', 'sections_length', 'text', 'text_length', 'summary', 'summary_length', 'title', 'count_text', 'count_summary', 'prompt', 'input_ids', 'attention_mask', 'labels', 'count_tokens_input_ids', 'count_tokens_labels'],\n",
      "        num_rows: 24861\n",
      "    })\n",
      "    dev: Dataset({\n",
      "        features: ['id', 'congress', 'bill_type', 'bill_number', 'bill_version', 'sections', 'sections_length', 'text', 'text_length', 'summary', 'summary_length', 'title', 'count_text', 'count_summary', 'prompt', 'input_ids', 'attention_mask', 'labels', 'count_tokens_input_ids', 'count_tokens_labels'],\n",
      "        num_rows: 3116\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'congress', 'bill_type', 'bill_number', 'bill_version', 'sections', 'sections_length', 'text', 'text_length', 'summary', 'summary_length', 'title', 'count_text', 'count_summary', 'prompt', 'input_ids', 'attention_mask', 'labels', 'count_tokens_input_ids', 'count_tokens_labels'],\n",
      "        num_rows: 3051\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'train': 0, 'dev': 0, 'test': 3}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load dataset\n",
    "ds = load_from_disk(dataset_path=path_preprocessed)\n",
    "print(ds)\n",
    "ds.cleanup_cache_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b37850f5-dabe-409a-87dd-982ad6793a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# small sample\n",
    "# ds_train = ds['train'].select_columns(['input_ids', 'attention_mask', 'labels']).select(range(20))\n",
    "# ds_dev = ds['dev'].select_columns(['input_ids', 'attention_mask', 'labels']).select(range(20))\n",
    "\n",
    "# ds_train = ds['train'].select_columns(['input_ids', 'attention_mask', 'labels']).select(range(500))\n",
    "# ds_dev = ds['dev'].select_columns(['input_ids', 'attention_mask', 'labels']).select(range(100))\n",
    "\n",
    "# ds_train = ds['train'].select_columns(['input_ids', 'attention_mask', 'labels']).select(range(5000))\n",
    "# ds_dev = ds['dev'].select_columns(['input_ids', 'attention_mask', 'labels']).select(range(1000))\n",
    "\n",
    "ds_train = ds['train'].select_columns(['input_ids', 'attention_mask', 'labels'])\n",
    "ds_dev = ds['dev'].select_columns(['input_ids', 'attention_mask', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d282ee29-4518-4803-be32-9fa88c62b5be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 24861\n",
      "})\n",
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 3116\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(ds_train)\n",
    "print(ds_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "061bb800-eb98-44e2-9d18-6669928d7fd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [21603, 10, 1300, 28281, 3438, 13, 1015, 3, 26300, 3604, 11, 27254, 494, 41, 9, 61, 1212, 144, 494, 5568, 3, 25626, 599, 9, 61, 14296, 13, 8, 5034, 1212, 144, 24996, 1983, 41, 2658, 412, 5, 134, 5, 254, 5, 431, 4241, 599, 9, 61, 14296, 61, 19, 21012, 318, 5637, 57, 11214, 5637, 37, 7471, 11, 8722, 53, 5637, 599, 188, 61, 37, 7471, 3, 117, 6499, 16, 769, 6583, 9413, 41, 188, 61, 41, 9, 7, 9943, 57, 8986, 5637, 201, 57, 11214, 4199, 120, 21, 3438, 441, 224, 1015, 3, 117, 11, 10153, 57, 2651, 44, 8, 414, 8, 826, 10, 41, 279, 61, 37, 7471, 1522, 29560, 3, 9, 1015, 3193, 24235, 53, 6082, 7, 19890, 288, 12, 769, 6583, 9413, 41, 188, 61, 21, 59, 705, 145, 943, 1093, 11, 59, 72, 145, 1640, 1093, 13, 8, 1358, 13, 24235, 53, 8, 6082, 7, 5, 5, 41, 115, 61, 1908, 83, 8224, 494, 5568, 305, 599, 9, 61, 14296, 13, 8, 1908, 83, 8224, 7554, 24996, 1983, 41, 2658, 412, 5, 134, 5, 254, 5, 314, 5062, 599, 9, 61, 14296, 61, 19, 21012, 318, 5637, 57, 11214, 5637, 37, 7471, 11, 8722, 53, 5637, 599, 188, 61, 37, 7471, 3, 117, 6499, 16, 769, 6583, 9413, 41, 188, 61, 41, 9, 7, 9943, 57, 8986, 5637, 201, 57, 11214, 4199, 120, 21, 3438, 441, 224, 1015, 3, 117, 11, 10153, 57, 2651, 44, 8, 414, 8, 826, 10, 41, 279, 61, 37, 7471, 1522, 29560, 3, 9, 1015, 3193, 24235, 53, 6082, 7, 19890, 288, 12, 769, 6583, 9413, 41, 188, 61, 21, 59, 705, 145, 943, 1093, 11, 59, 72, 145, 1640, 1093, 13, 8, 1358, 13, 24235, 53, 8, 6082, 7, 5, 5, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [736, 989, 7, 8, 5034, 1212, 144, 24996, 1983, 11, 8, 1908, 83, 8224, 7554, 24996, 1983, 12, 27765, 8, 1413, 5540, 3438, 13, 538, 3, 26300, 3604, 11, 27254, 3, 99, 8, 7471, 13, 16943, 2082, 7, 24, 538, 6082, 1502, 33, 44, 709, 4081, 12, 2822, 6082, 1502, 5, 7740, 7, 11807, 538, 23960, 21, 6082, 1358, 5, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "========== text ==========\n",
      "1. Interstate distribution of State inspected meat and poultry products \n",
      "(a) Meat products \n",
      "Section 301(a)(1) of the Federal Meat Inspection Act (21 U.S.C. 661(a)(1)) is amended— (1) by striking (1) The Secretary and inserting (1)(A) The Secretary ; (2) in subparagraph (A) (as designated by paragraph (1)), by striking solely for distribution within such State ; and (3) by adding at the end the following: (B) The Secretary shall reimburse a State agency administering inspections pursuant to subparagraph (A) for not less than 50 percent and not more than 60 percent of the costs of administering the inspections.. (b) Poultry products \n",
      "Section 5(a)(1) of the Poultry Products Inspection Act (21 U.S.C. 454(a)(1)) is amended— (1) by striking (1) The Secretary and inserting (1)(A) The Secretary ; (2) in subparagraph (A) (as designated by paragraph (1)), by striking solely for distribution within such State ; and (3) by adding at the end the following: (B) The Secretary shall reimburse a State agency administering inspections pursuant to subparagraph (A) for not less than 50 percent and not more than 60 percent of the costs of administering the inspections..\n",
      "========== summary ==========\n",
      "Amends the Federal Meat Inspection Act and the Poultry Products Inspection Act to authorize the interstate distribution of state inspected meat and poultry if the Secretary of Agriculture determines that state inspection requirements are at least equal to federal inspection requirements.\n",
      "\n",
      " Provides partial state reimbursement for inspection costs.\n"
     ]
    }
   ],
   "source": [
    "# access record\n",
    "print(ds_train[0])\n",
    "print_example(example=ds['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c4aaf1c5-374b-49fa-9b8c-59e97b087492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': Value(dtype='string', id=None), 'congress': Value(dtype='int64', id=None), 'bill_type': Value(dtype='string', id=None), 'bill_number': Value(dtype='int64', id=None), 'bill_version': Value(dtype='string', id=None), 'sections': [{'text': Value(dtype='string', id=None), 'id': Value(dtype='string', id=None), 'header': Value(dtype='string', id=None)}], 'sections_length': Value(dtype='int64', id=None), 'text': Value(dtype='string', id=None), 'text_length': Value(dtype='int64', id=None), 'summary': Value(dtype='string', id=None), 'summary_length': Value(dtype='int64', id=None), 'title': Value(dtype='string', id=None), 'count_text': Value(dtype='int64', id=None), 'count_summary': Value(dtype='int64', id=None), 'prompt': Value(dtype='string', id=None), 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None), 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None), 'labels': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None), 'count_tokens_input_ids': Value(dtype='int64', id=None), 'count_tokens_labels': Value(dtype='int64', id=None)}\n",
      "<class 'datasets.arrow_dataset.Dataset'>\n"
     ]
    }
   ],
   "source": [
    "print(ds['train'].features)\n",
    "print(type(ds['train']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d937638-e2ec-4060-8f6a-8ba224dfd9d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': Value(dtype='string', id=None),\n",
       " 'congress': Value(dtype='int64', id=None),\n",
       " 'bill_type': Value(dtype='string', id=None),\n",
       " 'bill_number': Value(dtype='int64', id=None),\n",
       " 'bill_version': Value(dtype='string', id=None),\n",
       " 'sections': [{'text': Value(dtype='string', id=None),\n",
       "   'id': Value(dtype='string', id=None),\n",
       "   'header': Value(dtype='string', id=None)}],\n",
       " 'sections_length': Value(dtype='int64', id=None),\n",
       " 'text': Value(dtype='string', id=None),\n",
       " 'text_length': Value(dtype='int64', id=None),\n",
       " 'summary': Value(dtype='string', id=None),\n",
       " 'summary_length': Value(dtype='int64', id=None),\n",
       " 'title': Value(dtype='string', id=None),\n",
       " 'count_text': Value(dtype='int64', id=None),\n",
       " 'count_summary': Value(dtype='int64', id=None),\n",
       " 'prompt': Value(dtype='string', id=None),\n",
       " 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None),\n",
       " 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None),\n",
       " 'labels': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None),\n",
       " 'count_tokens_input_ids': Value(dtype='int64', id=None),\n",
       " 'count_tokens_labels': Value(dtype='int64', id=None)}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['train'].features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5eef4fd-aed5-4b02-ab88-5b96124a89de",
   "metadata": {},
   "source": [
    "## Load model and set up for QLoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "839a56b0-97fa-45ba-a255-92f6d33beb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing the compute dtype\n",
    "def create_bnb_config() -> BitsAndBytesConfig:\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type='nf4',\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    )\n",
    "    return bnb_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8062e83d-dbe4-42fe-b61c-c7e58c798c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: later check data types for function signature\n",
    "def load_model(model_checkpoint: str, bnb_config: BitsAndBytesConfig) -> tuple[AutoModelForSeq2SeqLM, AutoTokenizer]:\n",
    "    n_gpus = torch.cuda.device_count()\n",
    "    # specific to GPU model\n",
    "    max_memory = f'{4096*1}MB'\n",
    "    \n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "        model_checkpoint, \n",
    "        quantization_config=bnb_config,\n",
    "        # dispatch efficiently the model on the available resources\n",
    "        device_map='auto',\n",
    "        max_memory={idx: max_memory for idx in range(n_gpus)}        \n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65dfaaed-d14d-48b2-b62d-89e10c5f546e",
   "metadata": {},
   "source": [
    "## Create bitsandbytes configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ca11cceb-446a-4af4-bc34-7220b8f84ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bnb_config = create_bnb_config()\n",
    "# print(bnb_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9ab907-c1db-4a37-917b-cbd090414ae8",
   "metadata": {},
   "source": [
    "## LoRA configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d89476b-2cc6-40f8-a4cc-b336e648d8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lora_config(\n",
    "    modules: torch.nn.Module, \n",
    "    rank=8, lora_alpha=16, lora_dropout=0.05, task_type=TaskType.SEQ_2_SEQ_LM\n",
    ") -> LoraConfig:\n",
    "    \"\"\"Apply LoRA to modules\"\"\"\n",
    "    # TODO: parameter, 8, 16, drop_out task_type\n",
    "    config = LoraConfig(\n",
    "        # rank of the update matrics, independent of r when use all linear layers in q-lora. \n",
    "        # may work on 8 then increase to 16 to see improvement\n",
    "        r=rank,  # \n",
    "        # parameter for scaling\n",
    "        lora_alpha=lora_alpha,  # 64,\n",
    "        target_modules=modules,\n",
    "        # dropout probability for layers. 0.05 for small models (7B, 13B), 0.1 for larger models (33B, 65B) from Q-LORA paper\n",
    "        lora_dropout=lora_dropout,  \n",
    "        bias='none',\n",
    "        # 'SEQ_2_SEQ_LM'\n",
    "        task_type=task_type,        \n",
    "    )\n",
    "    return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dab4ce34-0d1d-4364-aac7-6e910b55c208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all linear modules to apply lora, qlora all moduls as opposed to self-attention for lora only.\n",
    "def find_all_linear_names(model: PreTrainedModel) -> list:\n",
    "    # Q-LORA paper suggests that include all linear modules\n",
    "    model_modules = str(model.modules)\n",
    "    pattern = r'\\((\\w+)\\): Linear'\n",
    "    linear_layer_names = re.findall(pattern, model_modules)\n",
    "    \n",
    "    lora_module_names = set()\n",
    "    # Print the names of the Linear layers\n",
    "    for name in linear_layer_names:\n",
    "        lora_module_names.add(name)\n",
    "    target_modules = list(lora_module_names)\n",
    "    return target_modules   \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5a06bb0c-0202-4647-98a6-3cfc877cf778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print trainable parameters\n",
    "def print_trainable_parameters(model: torch.nn.Module, use_4bit=False):\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        # numel method inherited from torch.Tensor\n",
    "        num_params = param.numel()\n",
    "        # if using DS (deepspeed) Zero 3 and the weights are initialized empty\n",
    "        if num_params == 0 and hasattr(param, 'ds_numel'):\n",
    "            num_params = param.ds_numel\n",
    "        all_param += num_params\n",
    "        if param.requires_grad:\n",
    "            trainable_params += num_params\n",
    "    # # why devide by 2?\n",
    "    # if use_4bit:\n",
    "    #     trainable_params /= 2\n",
    "    trainable_perc = 100 * trainable_params / all_param\n",
    "    print(\n",
    "        f'all params: {all_param: d}, trainable params: {int(trainable_params): d}, trainable: {trainable_perc: .4f}%.'\n",
    "    )\n",
    "# Don't merge LORA adapter into a 4-bit LLM with Q-lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b0643118-4405-4c1d-9fcf-f070b98d8272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.is_available() should be True; otherwise, import error\n",
    "# model, tokenizer = load_model(model_checkpoint=model_checkpoint, bnb_config=bnb_config)\n",
    "\n",
    "# for LORA: load raw model\n",
    "def load_model_for_lora(model_checkpoint: str) -> tuple[AutoModelForSeq2SeqLM, AutoTokenizer]:\n",
    "    n_gpus = torch.cuda.device_count()\n",
    "    # specific to GPU model\n",
    "    max_memory = f'{4096*1}MB'\n",
    "    \n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "        model_checkpoint, \n",
    "        # quantization_config=bnb_config,\n",
    "        # dispatch efficiently the model on the available resources\n",
    "        device_map='auto',\n",
    "        max_memory={idx: max_memory for idx in range(n_gpus)}, \n",
    "        torch_dtype=torch.bfloat16\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "    return model, tokenizer\n",
    "\n",
    "# load original model and tokenizer\n",
    "original_model, tokenizer = load_model_for_lora(model_checkpoint=model_checkpoint)\n",
    "    \n",
    "# torch.nn.modules.module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a3b9d4e6-adf2-4fbc-a9a0-5f56b02ef3c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['q', 'v']\n"
     ]
    }
   ],
   "source": [
    "# lora_module_names = find_all_linear_names(model)\n",
    "\n",
    "# for lora\n",
    "lora_module_names = ['q', 'v']  \n",
    "print(lora_module_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c461f0df-41e3-41ea-82a5-99c2f3f01324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all params:  247577856, trainable params:  247577856, trainable:  100.0000%.\n"
     ]
    }
   ],
   "source": [
    "# print_trainable_parameters(model, use_4bit=True)\n",
    "print_trainable_parameters(original_model, use_4bit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "019f4908-2148-4582-bbc1-9fe47406c292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1243\n",
      "155\n"
     ]
    }
   ],
   "source": [
    "print(len(ds_train) //2 //10)\n",
    "print(len(ds_dev) //2 //10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4753826b-b505-4dba-9823-738cb0e69f12",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# torch.nn.modules.module\n",
    "# transformers.PretrainedModel\n",
    "def train(model, tokenizer, train_dataset, eval_dataset, checkpoint_dir, output_dir, max_target_length=512):\n",
    "    # note: model changes dynamically, modify input model\n",
    "    # Apply preprocessing to the model to prepare it by\n",
    "    # # 1 - Enabling gradient checkpointing to reduce memory usage during fine-tuning.\n",
    "    # # added use_reentrant = False to avoid warning\n",
    "    # model.gradient_checkpointing_enable(gradient_checkpointing_kwargs={'use_reentrant': False})\n",
    "\n",
    "    # 2 - Using the prepare_model_fot_kbit_training method from PEFT\n",
    "    # does not need this for LORA\n",
    "    # model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "    # Get lora module names\n",
    "    # modules = find_all_linear_names(model)\n",
    "    modules = ['q', 'v']  # attention layer\n",
    "    print(modules)\n",
    "\n",
    "    # Create PEFT config for these modules and wrap the model to PEFT\n",
    "    # peft_config = create_lora_config(modules)\n",
    "    # LoRA\n",
    "    peft_config = create_lora_config(modules, rank=16, lora_alpha=16)  \n",
    "    # peft_config = create_lora_config(modules, lora_dropout=0)\n",
    "\n",
    "    # note that this change model: running twice will make an error\n",
    "    peft_model = get_peft_model(model, peft_config)\n",
    "\n",
    "    # print information about the percentage of trainable parameters\n",
    "    print_trainable_parameters(peft_model)\n",
    "\n",
    "    # 1 - Enabling gradient checkpointing to reduce memory usage during fine-tuning.\n",
    "    # added use_reentrant = False to avoid warning\n",
    "    # model.gradient_checkpointing_enable(gradient_checkpointing_kwargs={'use_reentrant': False})\n",
    "\n",
    "    \n",
    "    # add trainining part\n",
    "    # Training parameters\n",
    "    batch_size = 8  # based on memory constraint, reduce batch size from 8 to 4\n",
    "    num_train_epochs = 8 * 3 # 8 * 5 # 2 # 8  # 16: 3 3.97 epochs\n",
    "    # show the training loss with every epoch\n",
    "    num_steps_for_epoch = len(train_dataset) // batch_size\n",
    "    num_steps_for_epoch_eval = len(eval_dataset) // batch_size\n",
    "    print(f'num_steps_for_epoch: {num_steps_for_epoch}')\n",
    "    print(f'num_steps_for_epoch_eval: {num_steps_for_epoch_eval}')\n",
    "\n",
    "    peft_training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=checkpoint_dir,\n",
    "        evaluation_strategy='epoch',\n",
    "        num_train_epochs=num_train_epochs,            \n",
    "        # evaluation_strategy='steps',\n",
    "        # save_strategy='steps',            \n",
    "        max_steps=num_train_epochs * num_steps_for_epoch,\n",
    "        # per_device_train_batch_size=batch_size,\n",
    "        # per_device_eval_batch_size=batch_size,\n",
    "        logging_steps=1,   # num_steps_for_epoch // 10,\n",
    "        # eval_steps=num_steps_for_epoch_eval // 10,  # _eval\n",
    "        learning_rate=1e-3, # Higher learning rate than full fine-tuning  # 2e-4,\n",
    "        auto_find_batch_size=True,\n",
    "        # weight_decay=0.01,            \n",
    "        # # pass\n",
    "        # gradient_accumulation_steps=4,  # effective batch size: batch_size * gradient_accumulation_steps\n",
    "        # eval_accumulation_steps=4,\n",
    "        # # gradient checking point\n",
    "        # gradient_checkpointing=True,\n",
    "        # gradient_checkpointing_kwargs={'use_reentrant': False},\n",
    "        # # seq2seq model\n",
    "        predict_with_generate=True,\n",
    "        # # have to explicitly specify this. setting by generation_config does not work.\n",
    "        # generation_max_length=max_target_length,\n",
    "        # generation_num_beams=5, \n",
    "        # generation_config=GenerationConfig(\n",
    "        #     max_new_tokens=max_target_length, num_beams=5,  \n",
    "        #     skip_special_tokens=True, early_stopping=False), \n",
    "        # # check point\n",
    "        # load_best_model_at_end=True,\n",
    "        # # should be multiple of eval_steps\n",
    "        # save_steps=num_steps_for_epoch_eval // 10 * 2 ,\n",
    "        # # precision\n",
    "        # fp16=True,            \n",
    "        # # need to verify this supports this optim            \n",
    "        # optim='adamw_bnb_8bit',\n",
    "        # # parallel processing\n",
    "        # dataloader_num_workers=4,             \n",
    "        # # output_dir='outputs',            \n",
    "        # # optim='paged_adamw_8bit',\n",
    "        # # warmup_steps=2,\n",
    "        # # max_steps=20,          \n",
    "    )\n",
    "    print(peft_training_args)\n",
    "    \n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=peft_model,\n",
    "        train_dataset=train_dataset, \n",
    "        eval_dataset=eval_dataset,\n",
    "        # verify all these arguments\n",
    "        args=peft_training_args, \n",
    "        # compute_metrics=compute_metrics_for_training,\n",
    "        # data_collator=Databfloat16CollatorForSeq2Seq(tokenizer, model=model),        \n",
    "    )\n",
    "    \n",
    "    # re-enable for inference to speed up predictions for similar inputs\n",
    "    peft_model.config.use_cache = False\n",
    "\n",
    "    ### SOURCE https://github.com/artidoro/qlora/blob/main/qlora.py\n",
    "    # Verifying the datatypes before training\n",
    "    dtypes = {}\n",
    "    for _, p in peft_model.named_parameters():\n",
    "        dtype = p.dtype\n",
    "        if dtype not in dtypes: \n",
    "            dtypes[dtype] = 0\n",
    "        dtypes[dtype] += p.numel()\n",
    "    total = 0\n",
    "    for data_type, num_param in dtypes.items():\n",
    "        total += num_param\n",
    "    for data_type, num_param in dtypes.items():\n",
    "        print(f'data type: {data_type}, num_param: {num_param}, percentage: {num_param/total * 100: .2f}')\n",
    "\n",
    "    do_train = True\n",
    "\n",
    "    # Launch training\n",
    "    print('training...')\n",
    "\n",
    "    if do_train:\n",
    "        train_result = trainer.train()\n",
    "        metrics = train_result.metrics\n",
    "        trainer.log_metrics('train', metrics)\n",
    "        trainer.save_metrics('train', metrics)\n",
    "        trainer.save_state()\n",
    "        print(metrics)\n",
    "\n",
    "    ###\n",
    "    # Saving model\n",
    "    print('Saving last checkpoint of the model...')\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    trainer.model.save_pretrained(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "    # Free memory for merging weights\n",
    "    del peft_model\n",
    "    del trainer\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a0bb6987-1c7b-4876-bcb3-5997c398447e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c3efab12-c64d-40d7-9df2-463f04c1b50d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../models/flan_t5_finetuned_lora/final_checkpoint-1703563820\n"
     ]
    }
   ],
   "source": [
    "checkpoint_dir = '../models/flan_t5_finetuned_lora/checkpoint'\n",
    "output_dir = f'../models/flan_t5_finetuned_lora/final_checkpoint-{str(int(time.time()))}'\n",
    "print(output_dir)\n",
    "# train(model, tokenizer, train_dataset=None, eval_dataset=None, output_dir=output_dir)\n",
    "# # only 3.2945% compared to 25%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05efb9c-5be2-4a0c-9f13-a39da612d637",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "90f4fc24-e2af-4661-bf49-a53704cd1f56",
   "metadata": {},
   "source": [
    "## Train: train_runtime = 7:39:09.53"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d5539cb8-1895-4662-b668-cfd7cbbb954d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['q', 'v']\n",
      "all params:  249347328, trainable params:  1769472, trainable:  0.7096%.\n",
      "num_steps_for_epoch: 3107\n",
      "num_steps_for_epoch_eval: 389\n",
      "Seq2SeqTrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=True,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=epoch,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "generation_config=None,\n",
      "generation_max_length=None,\n",
      "generation_num_beams=None,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=0.001,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=../models/flan_t5_finetuned_lora/checkpoint/runs/Dec25_20-10-49_stephen-MS-7C91,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=1,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=74568,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=24,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "output_dir=../models/flan_t5_finetuned_lora/checkpoint,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=8,\n",
      "predict_with_generate=True,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=../models/flan_t5_finetuned_lora/checkpoint,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=True,\n",
      "save_steps=500,\n",
      "save_strategy=steps,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "sortish_sampler=False,\n",
      "split_batches=False,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "data type: torch.bfloat16, num_param: 247577856, percentage:  99.29\n",
      "data type: torch.float32, num_param: 1769472, percentage:  0.71\n",
      "training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='74568' max='74568' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [74568/74568 7:39:09, Epoch 5/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.017700</td>\n",
       "      <td>0.094296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.051300</td>\n",
       "      <td>0.084767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.022200</td>\n",
       "      <td>0.081937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.320300</td>\n",
       "      <td>0.081767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.022800</td>\n",
       "      <td>0.078160</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =         6.0\n",
      "  total_flos               = 191720570GF\n",
      "  train_loss               =        0.12\n",
      "  train_runtime            =  7:39:09.53\n",
      "  train_samples_per_second =       5.413\n",
      "  train_steps_per_second   =       2.707\n",
      "{'train_runtime': 27549.5311, 'train_samples_per_second': 5.413, 'train_steps_per_second': 2.707, 'total_flos': 2.058583953993892e+17, 'train_loss': 0.11998937406479551, 'epoch': 6.0}\n",
      "Saving last checkpoint of the model...\n"
     ]
    }
   ],
   "source": [
    "# whole sample\n",
    "# max_steps some number instead of 1\n",
    "# rank=16 and alpha=16\n",
    "# train(model, tokenizer, train_dataset=ds_train, eval_dataset=ds_dev, output_dir=output_dir, max_target_length=max_target_length)\n",
    "train(\n",
    "    model=original_model, tokenizer=tokenizer, train_dataset=ds_train, eval_dataset=ds_dev, \n",
    "    checkpoint_dir=checkpoint_dir, output_dir=output_dir, max_target_length=max_target_length,\n",
    ")\n",
    "# only 3.2945% compared to 25%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f212fb-1e5c-453a-a1d8-524e6c1c799c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "24839aaa-f3ca-444f-b52a-0b0da00ef1b2",
   "metadata": {},
   "source": [
    "## Load tokenizer and trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bd09a06c-cf4d-4405-9112-38f0d9d9efbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all params:  249347328, trainable params:  0, trainable:  0.0000%.\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "# is_trainable=False for inference\n",
    "peft_model = PeftModel.from_pretrained(\n",
    "    original_model, output_dir, is_trainable=False, \n",
    "    # bfloat16 to save memory\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    # dispatch efficiently the model on the available resources\n",
    "    device_map='auto',\n",
    ")\n",
    "print_trainable_parameters(peft_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b7095b-d84d-44fe-bc7a-a47522bd5386",
   "metadata": {},
   "source": [
    "## Inference on entire data: it will take time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "329dff08-4d27-4593-85e6-d77f22ad29f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(\n",
    "    example: dict, tokenizer: AutoTokenizer, model: AutoModelForSeq2SeqLM, \n",
    "    max_target_length, \n",
    "    num_beams: int = 5, max_summary_ratio: float = 1.0, device: str = 'cpu'\n",
    ") -> dict:\n",
    "    \"\"\"Make inference.\n",
    "    :param example: dict, example.\n",
    "    :param tokenizer: transformers.AutoTokenizer, tokenizer.\n",
    "    :param model: transformers.AutoModelForSeq2SeqLM, model.\n",
    "    :param num_beams: int, the number of beams for beam search. Default: 5.\n",
    "    :param max_summary_ratio: float, the maximum summary to text ratio. Default: 1.\n",
    "    :param device: str, device to make an inference.\n",
    "    :return: dict, prediction with 'predicted_tokens' and 'prediction' columns.\n",
    "    \"\"\"\n",
    "    # convert dataset into tensor, need to put another dimension\n",
    "    # type torch.LongTensor or torch.int64\n",
    "    # torch batch does not work because length of the sequence varies with examples\n",
    "    input_ids = ( \n",
    "        torch.tensor(example['input_ids'])\n",
    "        .reshape((1, -1))\n",
    "        .to(device)\n",
    "    )\n",
    "    \n",
    "    # generate config\n",
    "    generation_config = GenerationConfig(\n",
    "        # does not need max_summary_ratio for count_tokens_input_ids after zero_padding: \n",
    "        # max_new_tokens=min(\n",
    "        #     int(max_summary_ratio * example['count_tokens_input_ids']),\n",
    "        #     max_target_length\n",
    "        # ),\n",
    "        max_new_tokens=max_target_length,\n",
    "        num_beams=num_beams, \n",
    "        skip_special_tokens=True, early_stopping=False\n",
    "    )\n",
    "    \n",
    "    # output type: torch.Tensor\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(input_ids=input_ids, generation_config=generation_config)\n",
    "    # back to cpu\n",
    "    if device != 'cpu':\n",
    "        outputs = outputs.to('cpu')\n",
    "        input_ids = input_ids.to('cpu')\n",
    "    # decode and remove the initial <pad> and the ending </s>\n",
    "    decoded_outputs = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # assign\n",
    "    example['predicted_tokens'] = outputs[0].tolist()\n",
    "    example['prediction'] = decoded_outputs\n",
    "    # clear cache\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9a8455fd-17b8-4560-80d5-5ef2c6e91d6d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "# peft_model.to(device)\n",
    "peft_model.device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48f7725-3620-4d22-b9ec-ffe432336ffe",
   "metadata": {},
   "source": [
    "* Took 1:45:43 for test dataset with batch size 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5209641-e10a-4045-9766-e0efa5aa5179",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "af1874b1-c8ff-483b-bb27-9f1548b02da6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2567591868674e3a84e80f3a95fc0662",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3051 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds_predict_test = ( \n",
    "    # ds['train']\n",
    "    # ds\n",
    "    ds['test']    \n",
    "    # ds['test'].select(range(500))  \n",
    "    .map(\n",
    "        inference, \n",
    "        fn_kwargs={\n",
    "            'tokenizer': tokenizer, 'model': peft_model, 'num_beams': num_beams, \n",
    "            'max_target_length': max_target_length, \n",
    "            'max_summary_ratio': max_summary_ratio, 'device': device,\n",
    "        }, \n",
    "        \n",
    "        # multiprocessing not working for gpu\n",
    "        num_proc=1,\n",
    "        batch_size=1024,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "363cbf96-9bff-42ee-b66b-a043b27bc7d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    test: Dataset({\n",
      "        features: ['id', 'congress', 'bill_type', 'bill_number', 'bill_version', 'sections', 'sections_length', 'text', 'text_length', 'summary', 'summary_length', 'title', 'count_text', 'count_summary', 'prompt', 'input_ids', 'attention_mask', 'labels', 'count_tokens_input_ids', 'count_tokens_labels', 'predicted_tokens', 'prediction'],\n",
      "        num_rows: 3051\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# combined with tokenized_train\n",
    "ds_predict_all = DatasetDict(\n",
    "    {\n",
    "        # 'train': ds_predict_train, \n",
    "        # 'dev': ds_predict_dev, \n",
    "        'test': ds_predict_test\n",
    "    }\n",
    ")\n",
    "print(ds_predict_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea071eb3-0196-4bd0-b851-b6777edafd2d",
   "metadata": {},
   "source": [
    "## Save predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "52bab676-a690-4a4c-8ef5-e17f312ddc13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73772e8c9f164ddd9f78593c5a057324",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/3051 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_dict.json  dev\ttest\n"
     ]
    }
   ],
   "source": [
    "ds_predict_all.save_to_disk(path_predictions)\n",
    "!ls {path_predictions}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e085faea-542b-428e-90a7-3901ecd73095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    test: Dataset({\n",
      "        features: ['id', 'congress', 'bill_type', 'bill_number', 'bill_version', 'sections', 'sections_length', 'text', 'text_length', 'summary', 'summary_length', 'title', 'count_text', 'count_summary', 'prompt', 'input_ids', 'attention_mask', 'labels', 'count_tokens_input_ids', 'count_tokens_labels', 'predicted_tokens', 'prediction'],\n",
      "        num_rows: 3051\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test': 0}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load dataset\n",
    "ds_predict_all = load_from_disk(dataset_path=path_predictions)\n",
    "print(ds_predict_all)\n",
    "ds_predict_all.cleanup_cache_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "47720b51-13d5-4fa8-80c6-af04f7a8b05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds_predict_test = ds_predict_all['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cf5c2bec-7147-4f88-bba1-4734d2262821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# model back to device\n",
    "if device != 'cpu':\n",
    "    peft_model.to('cpu')\n",
    "print(peft_model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95476f89-3d3d-428c-bb3e-a146a5e869c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3576c744-dedf-4533-bc35-adf72bd07906",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a27c6d-5cc3-4209-9001-b5134d3628f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d4d04b-1a85-416d-bd43-2be11a2eb00e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b47d7d97-d0c8-48df-84d4-acae270a5147",
   "metadata": {},
   "source": [
    "## Verify predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b11fc62a-5d5d-4612-8740-d5a3caaec385",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx: 0\n",
      "prompt\n",
      "summarize: That the House of Representatives— (1) reaffirms United States support for Georgia’s sovereignty and territorial integrity within its internationally-recognized borders, and does not recognize the independence of the Abkhazia and South Ossetia regions currently occupied by the Russian Federation; and (2) supports continued cooperation between the United States and Georgia and the efforts of the Government of Georgia to provide for the defense of its people and sovereign territory.\n",
      "\n",
      "prediction\n",
      "Reaffirms U.S. support for Georgia's sovereignty and territorial integrity within its internationally-recognized borders, and does not recognize the independence of the Abkhazia and South Ossetia regions currently occupied by the Russian Federation. Supports continued cooperation between the United States and Georgia and the efforts of the government of Georgia to provide for the defense of its people and sovereign territory.\n",
      "\n",
      "summary\n",
      "Expresses the sense that the House of Representatives: (1) reaffirms Georgia's sovereignty and territorial integrity within its internationally recognized borders, (2) does not recognize the independence of the Abkhazia and South Ossetia regions currently occupied by the Russian Federation, and (3) supports efforts to provide for Georgia's defense of its people and sovereign territory.\n",
      "\n",
      "idx: 1\n",
      "prompt\n",
      "summarize: 1. 3-Amino-4-methylbenzamide \n",
      "(a) In general \n",
      "Heading 9902.23.63 of the Harmonized Tariff Schedule of the United States (relating to 3-Amino-4-methylbenzamide) is amended by striking the date in the effective period column and inserting 12/31/2015. (b) Effective date \n",
      "The amendment made by this section applies to goods entered, or withdrawn from warehouse for consumption, on or after the 15th day after the date of the enactment of this Act.\n",
      "\n",
      "prediction\n",
      "Amends the Harmonized Tariff Schedule of the United States to extend the temporary suspension of duty on 3-Amino-4-methylbenzamide.\n",
      "\n",
      "summary\n",
      "Amends the Harmonized Tariff Schedule of the United States to extend the suspension of duty on 3-Amino-4-methylbenzamide.\n",
      "\n",
      "idx: 2\n",
      "prompt\n",
      "summarize: That the House of Representatives— (1) affirms the leadership and commitment of President Trump to simplifying the tax code for all Americans so that it is simple, fair, and easy to understand; (2) supports tax relief that will benefit Americans by raising their standard deduction, increasing the child tax credit, and lowering tax rates substantially; (3) supports individual rate cuts that touch all individuals and leverage higher rates of economic stimulus; (4) supports ending the rigged system where special interests win at the expense of the middle-class; (5) supports tax relief for all businesses of all sizes, by reducing rates as much as possible to restore America’s competitive edge and make America the best place in the world to hire, invest, and to grow; (6) supports family businesses by not allowing the death tax to crush the American dream; (7) supports retooling the tax code to give American workers a level playing field within our international economy; (8) supports switching from a worldwide tax system that encourages companies to keep their funds offshore to a territorial system that encourages companies to bring their profits back home to America; (9) encourages following regular order as tax reform moves forward to ensure the greatest likelihood of success; and (10) encourages all Americans to call on Congress and demand delivery of this once-in-a-generation opportunity to put America first.\n",
      "\n",
      "prediction\n",
      "(This measure has not been amended since it was introduced. The summary of that version is repeated here.) Expresses support for: (1) tax relief that will benefit Americans by raising their standard deduction, increasing the child tax credit, and lowering tax rates substantially; (2) individual rate cuts that touch all individuals and leverage higher rates of economic stimulus; (3) ending the rigged system where special interests win at the expense of the middle-class; (4) ending the rigged system where special interests win at the expense of the middle-class; (5) tax relief for all businesses of all sizes, by reducing rates as much as possible to restore America's competitive edge and make America the best place in the world to hire, invest, and grow; (6) ending the rigged system where special interests win at the expense of the middle-class; (7) ending the rigged system where special interests win at the expense of the middle-class; (5) ending the rigged system where special interests win at the expense of the middle-class; (5) ending the rigged system where special interests win at the expense of the middle-class; and (6) ending the rigged system where special interests win at the expense of the middle-class.\n",
      "\n",
      "summary\n",
      "Affirms the leadership and commitment of President Trump to simplify the tax code and supports tax policies such as raising the standard deduction, increasing the child tax credit, lowering rates, and changing the corporate tax system to a territorial system.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# idx = 0\n",
    "for idx in range(3):\n",
    "    print(f'idx: {idx}')\n",
    "    print(f\"prompt\\n{ds_predict_test['prompt'][idx]}\\n\")\n",
    "    print(f\"prediction\\n{ds_predict_test['prediction'][idx]}\\n\")\n",
    "    print(f\"summary\\n{ds_predict_test['summary'][idx]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacf634a-c5c3-4143-ab77-37faac7f0d6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c996eedc-c11c-41a7-9e8a-1294870f6d9c",
   "metadata": {},
   "source": [
    "## Evaulate performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0872c6e6-a976-4591-a0b6-3bbe7ea808b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = evaluate.load('rouge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e692f980-7790-4fa9-93dc-fcb5dac99e2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': 0.682, 'rouge2': 0.5868, 'rougeL': 0.645, 'rougeLsum': 0.6513}\n"
     ]
    }
   ],
   "source": [
    "result = compute_metrics((ds_predict_all['test']['prediction'], ds_predict_all['test']['summary']))\n",
    "# result = compute_metrics((ds_predict_test['prediction'], ds_predict_test['summary']))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486991a8-6e52-41aa-ae0e-c073607968b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179983e6-5e19-4827-ab38-94fc59e8a36d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143d3af0-2319-4929-97b4-f3099b47a1ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
